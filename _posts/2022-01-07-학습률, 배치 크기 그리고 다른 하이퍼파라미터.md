---
layout: post
categories: [Lab]
use_math: true
---

## 개요
- [개요](#개요)
- [학습률](#학습률)
- [옵티마이저](#옵티마이저)
- [배치 크기](#배치-크기)
- [반복 횟수](#반복-횟수)
- [마무리](#마무리-지으며)

학습과정에서 하이퍼파라미터를 설정하는 지침을 소개해드립니다.

## 학습률

일반적으로 최적의 학습률을 찾는 방법은 매우 낮은 학습률에서 시작해서 점진적으로 늘리는 겁니다.

예를 들면 $10^-5$부터 $10$까지 $\exp(log(10^6)/500)$를 한 번씩 곱하는 겁니다.

학습률에 따른 손실을 그래프로 그리면 처음에는 손실이 줄어들다가 손실이 다시 상승하는데

최적의 학습률은 손실이 다시 상승하는 지점보다 조금 아래로 대체로 상승점보다 10배 낮은 지점입니다.

## 옵티마이저

일반적인 경사 하강법에 다양한 기법을 더해 효율적인 경사 하강법을 제안합니다.

여러 옵티마이저가 있으나 일반적으로 AdamW를 사용하는 듯 합니다.

## 배치 크기

배치 크기는 2~32가 일반적이나 학습률 예열(warming up) 같은 기법을 사용하면 빠른 학습을 위해 훨씬 큰 배치도 사용가능합니다.

## 활성화 함수

일반적으로 ReLU 활성화 함수가 모든 은닉층에 좋은 기본값이나 수행하는 작업에 따라 바꿀 수 있습니다.

## 반복 횟수

따로 튜닝할 필요는 없으며 보통 조기 종료로 처리합니다.

## 마무리 지으며

신경망 하이퍼파라미터 튜닝에 관한 가장 좋은 [모범 사례](https://arxiv.org/abs/1803.09820)