---
layout: post
use_math: true
---


## 개요

신경망이 커질수록 성능이 좋아짐에도 불구하고 무한정으로 늘리지 못하는 이유가 있습니다.

- 그레디언트 소실과 폭주 문제
- 대규모 신경망을 위한 학습 데이터 부족
- 훈련 시간이 극단적으로 느려짐
- 훈련 세트에 과대적합될 문제

여기서는 근본적인 문제인 그레디언트 소실과 폭주 문제를 다루고자 합니다.

## 그레디언트 소실과 폭주

역전파 알고리즘은 출력층에서 입력층으로 오차 그레이디언트를 전파하면서 진행됩니다.

알고리즘이 신경망의 모든 파라미터에 대해 오차 함수의 그레디언틀 계산하면

경사 하강법 단계에서 이 그레이디언트를 사용하여 각 파라미터를 수정합니다.

하지만 알고리즘이 하위층으로 진행될수록 그레이디언트가 점점 작아지는 경우가 있습니다.

이것을 소실(vanishing)이라고 하고 반대로 점점 커지는 경우를 폭주(exploding)라고 합니다.

## 왜 생기는가?

그레디언트 소실은 2010년에 발표된 논문에 따르면 sigmoid 활성화 함수와 당시 가장 인기 있던 가중치 초기화 방법이 문제임을 밝혀냈습니다.

sigmoid 함수의 미분값의 최대값이 0.25인 1보다 작은 양수이기 때문에 

gradient가 0에 수렴하게되는 문제입니다.
